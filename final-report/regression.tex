\section{Regression}

\subsection{Introduction}

In this section we will find models that approximate the ordering of the adjectives when no comparisons between them are observed.  Towards this end, we will use logistic regression and elastic net regression to learn what it means for $s$ to be less than $t$. We construct a variety of feature representations using adverbs and/or phrases that co-occur with the adjectives for this task, and divide our annotated data into train, validation, and test sets to assess their efficacy. 

\subsection{Data Set}

Recall we have three sources of comparisons, N-gram data alone, PPDB data alone, and the combination of N-gram and PPDB data. For each data set, we extracted the pairs of adjectives where no data exists in corpus. The number of pairs for each annotated set for each data set is displayed in table 1. We will learn a different model for each data set (N-gram, PPDB, PPDB + N-gram) and assess their efficacy using their respective validation and test sets. 

\subsection{Literature Review}

Note in our graph we have 610 adverbs and phrases, and since we are interested in representing an adjective using its co-occurrence with adverbs/phrases, the feature representation could be large than the number of examples. Furthermore, this representation will be very sparse because most adjectives do not co-occur with most adverbs/phrases at all. Regularization will be necessary to prevent over-fitting. We consider two regularized models: $l_1$/$l_2$-penalized regression, and elastic net regression. In this section we will give a brief review of the two models.

Elastic net regression is a natural fit for our setting because it gives us the ability to select features and control sparsity. In this section we will give a brief overview of elastic net regression. Suppose our data set is $(\pmb{X}, \pmb{y})$ so that $\pmb{X}$ is the $n \times p$ design matrix, where each input $x$ is represented by the appropriate feature vector $\pmb{x} \in \mathbb{R}^p$, and let $\pmb{y}$ be the $n$-dimensional response vector $\pmb{y} = (y_1, \ldots, y_n)$. We assume $\pmb{y}$ is generated by this process:
	\[
		\pmb{y} = \pmb{X} \mathcal{\beta} + z,
	\]

where $z$ is a zero mean Gaussian noise factor. Then elastic net regression will recover the estimated $\widehat{\mathcal{\beta}}$ where:

	\[
		\widehat{\mathcal{\beta}} = \argmin_{\beta} \norm{\pmb{y} - \pmb{X} \sbeta} + \lambda_2 \norm{\sbeta}_2^2 + \lambda_1 \norm{\sbeta}_1.
	\]

Roughly, the $l_1$ penalty encourages a sparse solution where only a few variables in $\pmb{x}$ participate in predicting $\pmb{y}$, while the $l_2$ penalty encourages ``grouping" so that more than a few variables in $\pmb{x}$ participates. 

Now we review logistic and regularized logistic regression for binary outcomes. Given a $n \times p$ design matrix $\pmb{X}$, logistic regression models the vector of probability $\pmb{p}$ by:
	\[
		\log \frac{\pmb{p}}{1 - \pmb{p}} = \pmb{X}^T \mathcal{\beta},
	\]
and we see that $\pmb{p}$ is:
	\[
		\pmb{p} = \frac{exp(\pmb{X}^T \pmb{\beta})}{1 + exp(\pmb{X}^T \pmb{\beta})}.
	\]
Again given the binary outcome vector $\pmb{y} \in \{0,1\}^n$, the loss function $\mathcal{L}$ is:
	\[
		\mathcal{L}(\pmb{\beta}) = \log \pmb{p} + (1-\pmb{y})^T \log(1 - \pmb{p}),
	\]
we can find the best $\pmb{\beta}$ by:
	\[	
		\widehat{\pmb{\beta}} = \argmin_{\pmb{\beta}} \mathcal{L}
	\]

In our setting, we experiment with two penalties on $\pmb{\beta}$: $l_2$-penalty or ridge logistic regression, and $l_1$-penalty or LASSO logistic regression. In ridge logistic regression, we simply add the $l_2$-penalty to the objective function:
	\[	
		\mathcal{L}_{ridge}(\pmb{\beta}) = \mathcal{L} - \frac{1}{2} \lambda \norm{\pmb{\beta}}_2^2.
	\]
Similarly, for LASSO logistic regression, the objective function is:
	\[	
		\mathcal{L}_{LASSO}(\pmb{\beta}) = \mathcal{L} - \frac{1}{2} \lambda \norm{\pmb{\beta}}_1^2.
	\]
Again the ridge penalty encourages grouping of all variables, while LASSO encourages sparsity.

\subsection{Problem Formulation}

Now we will formulate our problem in terms of the two models we just introduced. In our setting, we will define:

\[   
y = \left\{
\begin{array}{ll}
      1 & s < t \\
      0 & otherwise.
\end{array} 
\right. 
\]

And for each pair of adjectives $s$ and $t$. Additionally, we will need to find a corresponding feature representation so that:

	\[
		\pmb{x} = g(\phi(s), \phi(t)),
	\]

for some function $\phi$ and $g : \mathbb{R}^m \times \mathbb{R}^m \rightarrow \mathbb{R}^p$, where $m \leq p$. In a departure from notation of previous chapter, $s$ now refers to the string representation of the word, while $\phi(s)$ is the corresponding vector representation. If we have this model $\widehat{\sbeta}$ and the appropriate $g$ and $\phi$, then we can use this definition.

\begin{definition}

Given words $s$ and $t$, and their representation $\pmb{x} = g(\phi(s), \phi(t))$, and let:
	\[
		\hat{y} = \widehat{\sbeta}^{T} \pmb{x},
	\]

where $\widehat{\sbeta}$ is the elastic net model, then we can define:

\[   
\Prob[s < x ] = \left\{
\begin{array}{ll}
      \frac{1}{2} + \epsilon & \hat{y} < \delta \\
      \frac{1}{2} - \epsilon & otherwise,
\end{array} 
\right. 
\]

for an appropriate threshold $\delta$. Again we discard the value of $\hat{y}$ and define the probability by fiat. In the case of penalized logistic regression, we use the actual probability output by the model.


\end{definition}

\subsection{Feature Representations}

In this section we describe two broad sets of features we use, and their associated function $g$. In the first set of features, we will represent the adjective by the frequency of adverbs incident and/or outgoing from this adjective. In the second set of features we will represent the adjective by adjectives that are its neighbors. In an attempt to avoid confusion, we will denote the first set of features $\phi(s)$, while the second set will be $\nu(s)$.

First we list the $\phi$'s.

\begin{enumerate}
	\item In-neighbor only. So that for each adverb $v$:

		\[   
		\phi(s)^{in}_v = \left\{
		\begin{array}{ll}
		      n & \text{there are n edges pointing to $s$ from all neighbors with the adverb v} \\
		      0 & otherwise.
		\end{array} 
		\right.
		\]

	\item Out-neighbor only. So that for each adverb $v$:

		\[   
		\phi(s)^{out}_v = \left\{
		\begin{array}{ll}
		      n & \text{there are n edges pointing from $s$ to all neighbors with the adverb v} \\
		      0 & otherwise.
		\end{array} 
		\right. 
		\]

	\item Concatenation of in and out neighbor. So that $\phi(s) = (\phi^{in}, \phi^{out})$. In this case we also considered $\phi(s) = (\phi^{in}, - \phi^{out})$, where $- \phi^{out}$ is scalar multiplication of $-1$ with all entries of $\phi^{out}$.

	\item Element wise addition of in and out neighbor. So that $\phi(s) = \phi^{in} + \phi^{out}$.
	
	\item Element wise subtraction of in and out neighbor. So that $\phi(s) = \phi^{in} - \phi^{out}$.

\end{enumerate}

Furthermore, for each $\phi^{in}$ and $\phi^{out}$, we can vary the number of adverbs in the vector. We sort the adverbs by frequency of appearance and pick the top 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, and all 605 adverbs/phrases. 

Now we consider the $\nu$'s. In this case we find all neighbors of $s$ and represent each neighbor by the frequency of adverbs between the neighbor and our vertex. We list the $\nu$'s.

\begin{enumerate}
	\item In-neighbor only. So that for each neighbor $t$:

		\[   
		\nu(s)^{in}_t = \left\{
		\begin{array}{ll}
		      n & \text{there are n edges pointing to $s$ from $t$} \\
		      0 & otherwise.
		\end{array} 
		\right.
		\]

	\item Out-neighbor only. So that for each neighbor $t$:

		\[   
		\nu(s)^{out}_t = \left\{
		\begin{array}{ll}
		      n & \text{there are n edges pointing from $s$ to $t$} \\
		      0 & otherwise.
		\end{array} 
		\right. 
		\]

	\item Concatenation of in and out neighbor. So that $\nu(s) = (\nu^{in}, \nu^{out})$. Again we also experiment with $\nu(s) = (\nu^{in}, -\nu^{out})$.

	\item Element wise addition of in and out neighbor. So that $\phi(s) = \phi^{in} + \phi^{out}$.
	
	\item Element wise subtraction of in and out neighbor. So that $\phi(s) = \phi^{in} - \phi^{out}$.

	\item Bernoulli representation of in and out neighbor. So that for each neighbor $t$ we have:

		\[   
			\nu(s)^{\mathcal{B}}_t = \left\{
			\begin{array}{ll}
			      \frac{|\{(t,s) \in \pmb{E}\}|}{|\{(t,s) \in \pmb{E}\}| + |\{(s,t) \in \pmb{E}\}|} & \text{there is at least one edge from $t$ to $s$} \\
			      \frac{1}{10} & otherwise.
			\end{array} 
			\right. 
		\]

\end{enumerate}

In all cases, we sort the neighbors so that the most connected neighbor to $s$ is at the top, and so on. Finally we take the top $n$ neighbors of $s$ as its feature representation, where $n$ is also an experimental parameter. If an adjective has less than $n$ neighbors, then we pad the vector with $0$. Note how we smooth out the definition of $\nu$ above so that the padding can be distinguished from a case where there are vertices from $s$ to $t$, but not vice versa.

Finally, we explore a variety of $g$'s:

\begin{enumerate}
	\item element wise addition. $\pmb{x} = \phi(s) + \phi(t)$,
	\item element wise subtraction. $\pmb{x} = \phi(s) - \phi(t)$,
	\item concatenation. $\pmb{x} = (\phi(s), \phi(t))$,
	\item dot product. $\pmb{x} = \phi(s) \cdot \phi(t)$,
\end{enumerate}

and similarly for $\nu$. After $g$ has been applied, we also experimented with using the raw counts, versus normalizing the entries of $\phi$ so they are between $0$ and $1$. Normalization appears to make sense when the N-gram data is combined with the PPDB data, since the N-gram data is often times three orders of magnitude larger than the PPDB data. We normalized the raw $n \times p$ design matrix with $n$ samples and $p$ features in one of two ways.

\begin{enumerate}
	\item Normalize by mean and variance. Suppose each feature is a unique multinomial random variable $\pmb{x} \in \{1,\ldots,N\}$ for some suitable $N$, then we can normalize this variable by:
		\[	
			\tilde{x} = \frac{x - \pmb{E}[\pmb{x}]}{var(\pmb{x})}.
		\]
	\item Normalize by maximum value. So we have $\tilde{x} = \frac{x}{max(x_1,\ldots,x_n)}$
\end{enumerate}


\begin{table}
\small
\centering
\begin{tabular}{|l|c|c|c|}
	% 
	\hline 
	& \multicolumn{1}{c|}{Mohit} 
	& \multicolumn{1}{c|}{Turk} 
	& \multicolumn{1}{c|}{BCS} \\
	\hline 
	% 
	N-gram          & 550 & 586 & 556 \\
	PPDB            & 750 & 182 & 294 \\
	PPDB + N-gram   & 408 & 170 & 290 \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table} The base-comparative-superlative pairs form the training set for each data set, the Turk pairs form the validations set, while Mohit's pairs will be the test set. Note in almost all cases but one, the test set is actually larger than than the training set. }
\end{table}


\subsection{Results}

In addition to varying the feature representations, we also varied the emphasis over each of the two penalty terms in the objective function. For the sake of brevity, we report results from the two best performing models in each method only: (1) $l_1$-penalized logistic regression model with Bernoulli representation of the top 10 most connected neighbors and (2) elastic-net regression where the feature vector is the Bernoulli representation of the top 50 most connected neighbors and. Note this model is learned using the data set with PPDB and N-gram comparisons and base-comparative-superlative gold sets, validated using the Turk set, and tested using Mohit's set.

\begin{table}
\small
\centering
\begin{tabular}{|l|cc|cc|cc|}
	% 
	\hline 
	& \multicolumn{2}{c|}{Elastic Net Regression } 
	& \multicolumn{2}{c|}{$l_1$-Logistic Regression} \\
	\hline 
	\bf Gold Set
	& \bf Pairwise & \bf Avg. $\tau$  
	& \bf Pairwise & \bf Avg. $\tau$  \\ 
	\hline
	% 
	BCS   & 77.0\% & 0.54 & 86.0\%  &  0.72 \\
	Turk  & 61.0\% & 0.22 & 61.0\%  &  0.22 \\
	Mohit & 71.0\% & 0.44 & 72.0\%  &  0.44 \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table}. Results for the two best models. Note the performance on the validation and test sets are comparable between the two models. It is also curious to note that the models performed better on the test set than the validation set. In fact, no model performed better than $70\%$ on the validation set. }
\end{table}

All models were implemented using Python's Scikit-learn library. The best performing $l_1$-penalized logistic regression model has a constant $C$ of $0.4$. While the best performing elastic net model has an $\alpha$ of $0.9$, and $l_1$ of $0.1$. 

One natural question we can ask is how well the model performs on the pairs of adjective with no direct comparisons, versus how well the baseline performs on the set of pairs with direct comparisons. Table 3 displays the baseline one pairs with direct comparisons. Overall, we see that the model does $9\%$ worse on the Turk set relative to the baseline using PPDB and N-gram data, and does only $2\%$ worse relative to the baseline on Mohit's clusters.



\begin{table}
\small
\centering
\begin{tabular}{|l|cc|cc|cc|}
	% 
	\hline 
	& \multicolumn{2}{c|}{N-gram} 
	& \multicolumn{2}{c|}{PPDB} 
	& \multicolumn{2}{c|}{PPDB + N-gram} \\
	\hline 
	\bf Test set
	& \bf Pairwise & \bf Avg. $\tau$ 
	& \bf Pairwise & \bf Avg. $\tau$ 
	& \bf Pairwise & \bf Avg. $\tau$ \\
	\hline
	% 
	BCS   & 100.0\% & 1.00 & 97.0\% &  0.93 & 97.0\% & 0.94 \\
	% 
	Turk  & 78.0\%  & 0.57 & 69.5\% &  0.38 & 70.0\% & 0.40 \\
	% 
	Mohit & 84.0\%  & 0.67 & 48.2\% & -0.04 & 74.3\% & 0.49 \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table} Results across all datasets for pairs of gold standards for which direct comparison exists. Note how performance is higher when using N-grams alone across all gold standards. Note how PPDB data alone fails to do better than random on Mohit's clusters, even though direct direct comparisons exists for the pairs in these clusters. Suggesting the data that exists for Mohit's pairs are too noisy to give useful information. }
\end{table}\newpage




























