\section{Regression}

\subsection{Introduction}

In the previous two chapters we attempted to construct generic measures that may work for every graph, but in the end certainly did not perform on this graph. In this chapter we will construct a measure specifically for this dataset. Towards this end, we will use elastic net regression to learn what it means for $s$ to be less than $t$. We construct a variety of feature representations using adverbs and/or phrases that co-occur with the adjectives for this task, and divide our annotated data into train, validation, and test sets to assess their efficacy. 

\subsection{Data Set}

Recall we have three sources of comparisons, N-gram data alone, PPDB data alone, and the combination of N-gram and PPDB data. For each data set, we extracted the pairs of adjectives where no data exists in corpus. The number of pairs for each annotated set for each data set is displayed in table 1. We will learn a different model for each data set (N-gram, PPDB, PPDB + N-gram) and assess their efficacy using their respective validation and test sets. 

\subsection{Literature Review}

Note in our graph we have 610 adverbs and phrases, and since we are interested in representing an adjective using its co-occurrence with adverbs/phrases, the feature representation could be large than the number of examples. Furthermore, this representation will be very sparse because most adjectives do not co-occur with most adverbs/phrases at all. Thus regularization will be necessary to prevent over-fitting. In this section we will give a brief overview of elastic net regression. Suppose our data set is $(\pmb{X}, \pmb{y})$ so that $\pmb{X}$ is the $n \times p$ design matrix, where each input $x$ is represented by the appropriate feature vector $\pmb{x} \in \mathbb{R}^p$, and let $\pmb{y}$ be the $n$-dimensional response vector $\pmb{y} = (y_1, \ldots, y_n)$. We assume $\pmb{y}$ is generated by this process:
	\[
		\pmb{y} = \pmb{X} \mathcal{\beta} + z,
	\]

where $z$ is a zero mean Gaussian noise factor. Then elastic net regression will recover the estimated $\widehat{\mathcal{\beta}}$ where:

	\[
		\widehat{\mathcal{\beta}} = \argmin_{\beta} \norm{\pmb{y} - \pmb{X} \sbeta} + \lambda_2 \norm{\sbeta}_2^2 + \lambda_1 \norm{\sbeta}_1.
	\]

Roughly, the $l_1$ penalty encourages a sparse solution where only a few variables in $\pmb{x}$ participate in predicting $\pmb{y}$, while the $l_2$ penalty encourages ``grouping" so that more than a few variables in $\pmb{x}$ participates. 

\subsection{Problem Formulation}

In our setting, we will define:

\[   
y = \left\{
\begin{array}{ll}
      1 & s < t \\
      0 & otherwise.
\end{array} 
\right. 
\]

And for each pair of adjectives $s$ and $t$. Additionally, we will need to find a corresponding feature representation so that:

	\[
		\pmb{x} = g(\phi(s), \phi(t)),
	\]

for some function $\phi$ and $g : \mathbb{R}^m \times \mathbb{R}^m \rightarrow \mathbb{R}^p$, where $m \leq p$. In a departure from notation of previous chapter, $s$ now refers to the string representation of the word, while $\phi(s)$ is the corresponding vector representation. If we have this model $\widehat{\sbeta}$ and the appropriate $g$ and $\phi$, then we can use this definition.

\begin{definition}

Given words $s$ and $t$, and their representation $\pmb{x} = g(\phi(s), \phi(t))$, and let:
	\[
		\hat{y} = \widehat{\sbeta}' \pmb{x},
	\]

then we can define:

\[   
\Prob[s < x ] = \left\{
\begin{array}{ll}
      \frac{1}{2} + \epsilon & \hat{y} < \delta \\
      \frac{1}{2} - \epsilon & otherwise,
\end{array} 
\right. 
\]

for an appropriate threshold $\delta$. Again we discard the value of $\hat{y}$ and define the probability by fiat.
\end{definition}

\subsection{Feature Representations}

In this section we describe two broad sets of features we use, and their associated function $g$. In the first set of features, we will represent the adjective by the frequency of adverbs incident and/or outgoing from this adjective. In the second set of features we will represent the adjective by adjectives that are its neighbors. In an attempt to avoid confusion, we will denote the first set of features $\phi(s)$, while the second set will be $\nu(s)$.

First we list the $\phi$'s.

\begin{enumerate}
	\item In-neighbor only. So that for each adverb $v$:

		\[   
		\phi(s)^{in}_v = \left\{
		\begin{array}{ll}
		      n & \text{there are n edges pointing to $s$ with the adverb v} \\
		      0 & otherwise.
		\end{array} 
		\right.
		\]

	\item Out-neighbor only. So that for each adverb $v$:

		\[   
		\phi(s)^{out}_v = \left\{
		\begin{array}{ll}
		      n & \text{there are n edges pointing from $s$ with the adverb v} \\
		      0 & otherwise.
		\end{array} 
		\right. 
		\]

	\item Concatenation of in and out neighbor. So that $\phi(s) = (\phi^{in}, \phi^{out})$. In this case we also considered $\phi(s) = (\phi^{in}, - \phi^{out})$, where $- \phi^{out}$ is scalar multiplication of $-1$ with all entries of $\phi^{out}$.

	\item Element wise addition of in and out neighbor. So that $\phi(s) = \phi^{in} + \phi^{out}$.
	
	\item Element wise subtraction of in and out neighbor. So that $\phi(s) = \phi^{in} - \phi^{out}$.

\end{enumerate}

Furthermore, for each $\phi^{in}$ and $\phi^{out}$, we can vary the number of adverbs in the vector. We sort the adverbs by frequency of appearance and pick the top 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, and all 605 adverbs/phrases. 

Now we consider the $\nu$'s. In this case we find all neighbors of $s$ and represent each neighbor by the frequency of 


Finally, we explore a variety of $g$'s:

\begin{enumerate}
	\item element wise addition. $\pmb{x} = \phi(s) + \phi(t)$,
	\item element wise subtraction. $\pmb{x} = \phi(s) - \phi(t)$,
	\item concatenation. $\pmb{x} = (\phi(s), \phi(t))$,
	\item dot product. $\pmb{x} = \phi(s) \cdot \phi(t)$.
\end{enumerate}

After $g$ has been applied, we also experimented with normalizing the entries of $\phi$ so they are between $0$ and $1$. 


\begin{table}
\small
\centering
\begin{tabular}{|l|c|c|c|}
	% 
	\hline 
	& \multicolumn{1}{c|}{Mohit} 
	& \multicolumn{1}{c|}{Turk} 
	& \multicolumn{1}{c|}{BCS} \\
	\hline 
	% 
	N-gram          & 550 & 586 & 556 \\
	PPDB            & 750 & 182 & 294 \\
	PPDB + N-gram   & 408 & 170 & 290 \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table} The base-comparative-superlative pairs form the training set for each data set, the Turk pairs form the validations set, while Mohit's pairs will be the test set. Note in almost all cases but one, the test set is actually larger than than the training set. }
\end{table}


\subsection{Results}

In addition to varying the feature representations, we also varied the emphasis over each of the two penalty terms in the objective function. All results are displayed in the tables below. Before a more nuanced discussion, we will highlight a few points:

\begin{itemize}
	\item Once again none of the methods passed the $\log(n)$ threshold we wished. In fact, no method performed above $71\%$ accuracy.
	\item 
\end{itemize}


\begin{table}
\small
\centering
\begin{tabular}{|l|ccc|ccc|ccc|}
	% 
	\hline 
	& \multicolumn{3}{c|}{N-gram} 
	& \multicolumn{3}{c|}{PPDB} 
	& \multicolumn{3}{c|}{PPDB + N-gram} \\
	\hline 
	\bf Test set
	& \bf Pairwise & \bf Avg. $\tau$ & \bf Avg. $|\tau|$ 
	& \bf Pairwise & \bf Avg. $\tau$ & \bf Avg. $|\tau|$ 
	& \bf Pairwise & \bf Avg. $\tau$ & \bf Avg. $|\tau|$ \\ 
	\hline
	% 
	Mohit & 00.0\%  & 0.00 & 0.00  & 00.0\% & 0.00 & 0.00 & 00.0\% & 0.00 & 0.00 \\ 
	% 
	Turk  & 00.0\%  & 0.00 & 0.00  & 00.0\% & 0.00 & 0.00 & 00.0\% & 0.00 & 0.00 \\
	% 
	BCS   & 00.0\%  & 0.00 & 0.00  & 00.0\% & 0.00 & 0.00 & 00.0\% & 0.00 & 0.00 \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table} }
\end{table}




















