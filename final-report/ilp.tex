\subsection{ILP over N-gram Patterns}

\subsection{Pairwise Ranking with One Sided Patterns}

Since $score(a_i, a_j)$ is zero for most pairs of adjectives due to lack of data, we are motivated to find alternate ways of approximating this value using single sided patterns of form: $\{a_i p *, a_j p *, * p a_i, * p a_j\}$. Loosely speaking, even if we do not observe any $a_i \: p \: a_j$ for some weak-strong pattern $p$, we can still approximate the liklihood of observing this string using the frequency in which $a_i$ appears in front of the the pattern $p$, and the frequecy $a_j$ appears behind the pattern $p$. Once we approximate the liklihood for $a_j \: p a_i$ over weak-strong patterns $p$, and similarly for all strong-weak patterns, we can infer whether adjective $a_i$ is weaker than $a_j$ by determining whether $a_i$ is more likely to appear on the weaker side of each phrase. This intuition is naturally expressed in the heuristic:

\[
  score(u) = \frac{ cnt(u \: p_{sw} \: *) + cnt(* \: p_{ws} \: u)  }
                  { cnt(u \: p_{ws} \: *) + cnt(* \: p_{sw} \: u) },
\]

where:
\[cnt( u \: p_{sw} \: *) = \sum_{v \in \mathbf{V}} \sum_{p \in P_{sw}} cnt( u \: p \: v).\]

This score captures the proportion of times $u$ dominates all other words through the patterns given, relative to the proportion of times $u$ is dominated by all other words through the pattern. The results of this ranking is displayed in table 6 in the line ``Markov heuristic."

Now we make the intuition precise. Suppose we have a simple language $\mathbf{L}$ made up only of phrases of the form ``word pattern word" for every word in the unigram set $\mathbf{V}$ and every pattern in table 1, that is we have:

\[ \mathbf{L} = \{ u \: p \: v \quad|\quad u,v \in \mathbf{V}, p \in \mathbf{P_{sw}} \cup \mathbf{P_{ws}} \}. \]

If we can confidently approximate liklihood of each phrase from $\mathbf{L}$ based on N-gram corpus evidence alone then we are done. But because data is sparse, we must fill in the missing counts by assuming the phrases in $\mathbf{L}$ is generated by this markov process involving two random variables, $V$ whose value range over vocabulary $\mathbf{V}$, and $P$ ranging over the patterns in table 1. The speaker selects a word $u$ according to some distribution $\mathcal{D}_{V}$ over $\mathbf{V}$,  then conditioned on the fact that $u$ is drawn, a phrase $p$ is drawn according to the conditional distribution $\mathcal{D}_{P|V}$. Finally, conditioned on the fact that $p$ is drawn, a word $v$ is sampled from $\mathcal{D}_{V|P}$. The attentive reader will observe that this crude model does not respect word order. In the phrase ``not great (,) just good", our model would generate the phrase ``good not just great". Surpringly this model works well enough to outperform Bansal' method. Now the probabilty of a phrase is:

\begin{align*}
\mathcal{D}_L = \frac{\mathcal{D}_{V} \: \mathcal{D}_{P|V} \: \mathcal{D}_{V|P}}{Z},\\
\end{align*}

where $Z$ is an appropriate normalization constant. But since we are only interested comparing the relative liklihood of phrases, $Z$ doese not need to be computed. So we have:

\begin{align*}
\mathcal{D}_L = Pr[u \: p \: v] \propto Pr[u] Pr[p | u] Pr[v | p], \quad (1)
\end{align*}

where:
\[Pr[V = u]               = \frac{cnt(u)}{cnt(*)}\]
\[Pr[P = p | V = u]  = \frac{cnt(u \: p \: *)}{cnt(u \: *)}\]
\[Pr[V = v | P = p]  = \frac{cnt(* \: p \: v)}{cnt(* \: p \: *)}\]

where $cnt(*) = \sum_{x \in \mathbf{V}} cnt(x)$. The first distribution is approximated by the one-gram corpus, the second and third distribution by four and five grams. In the interest of not computing normaliztion constant whenever possible, we put the following crude bound on $cnt(u \: *)$:

\begin{align*}
cnt(u \: *) &= \sum_{x} count(u \: x) \leq cnt(u),
\end{align*}

where $x$ is ranges over all suffixes of length three or four. So (1) becomes:\\

\begin{math}
Pr[u \: p \: v] \propto \frac{ cnt(u \: p \: *) \cdot cnt(* \: p \: v) }{ cnt(*) \cdot cnt(* \: p \: *)}. \quad (2) \\
\end{math}

Now define the probability that $u$ is stronger than $v$ under $\mathcal{D}_L$ as:
% define $u$ is probably stronger than $v$ under $\mathcal{D}_L$ as:

\begin{align*}
  Pr[u \: > \: v] &= Pr[ u \: P_{sw} \: v  \quad or \quad v \: P_{ws} \: u] \\
          &= Pr[ u \: P_{sw} \: v] + Pr[ v \: P_{ws} \: u] \\
          &= \sum_{p \in P_{sw}} Pr[u \: p \: v] + \sum_{p \in P_{ws}} Pr[v \: p \: u],
\end{align*}
and similarly for $v > u$. We decide $u$ is stronger than $v$ if:

\begin{align*}
  &Pr[u \: > \: v] \ge Pr[v \: > \: u] \\
  % & Pr[ u \: P_{sw} \: v  \quad or \quad v \: P_{ws} \: u] > Pr[ v \: P_{sw} \: u  \quad or \quad u \: P_{ws} \: v]\\
  % &\implies Pr[ u \: P_{sw} \: v] + Pr[ v \: P_{ws} \: u] > Pr[ v \: P_{sw} \: u] + Pr[ u \: P_{ws} \: v]\\
  &\implies \\
  &\frac{ cnt(u P_{sw} *) \cdot cnt(* P_{sw} v) }{ cnt(*) \cdot cnt(* P_{sw} *)} 
  + \frac{ cnt(v P_{ws} *) \cdot cnt(* P_{ws} u) }{ cnt(*) \cdot cnt(* P_{ws} *)} \\
  &\ge\\
  &\frac{ cnt(v P_{sw} *) \cdot cnt(* P_{sw} u) }{ cnt(*) \cdot cnt(* P_{sw} *)} 
  + \frac{ cnt(u P_{ws} *) \cdot cnt(* P_{ws} v) }{ cnt(*) \cdot cnt(* P_{ws} *)} \\
  % 
  &\implies\\
  % % 
  &\frac{ cnt(u P_{sw} *) \cdot cnt(* P_{sw} v) }{ cnt(* P_{sw} *)} 
  + \frac{ cnt(v P_{ws} *) \cdot cnt(* P_{ws} u) }{ cnt(* P_{ws} *)} \\
  &\ge\\
  &\frac{ cnt(v P_{sw} *) \cdot cnt(* P_{sw} u) }{ cnt(* P_{sw} *)} 
  + \frac{ cnt(u P_{ws} *) \cdot cnt(* P_{ws} v) }{ cnt(* P_{ws} *)}. \quad (3) \\
\end{align*}

Note the normalization constant $cnt(*)$ drops out, and there is a qualitative symmetry in (3) that echos intuition. Since (3) does not output cycles, ranking is done by topological sort; results are reported in table 6 under ``markov pairwise approximate."  Next, we combine the approximate value from (3) with those directly observed in the corpus. If for some adjective pair $(u,v)$ we observe any one of the following values: $u \: p_{sw} \: v$, $u \: p_{ws} \: v$, $v \: p_{sw} \: u$, or $v \: p_{ws} \: u$, then we can define the probability that $u > v$ under $\mathcal{D}_L$ as:

\begin{align*}
Pr[u > v] &= \frac{cnt(u \: P_{sw} \: v) + cnt(v \: P_{ws} \: u)}{Z} \quad (4)
\end{align*}

where 

\begin{align*}
Z &= cnt(u \: P_{sw} \: v) + cnt(v \: P_{ws} \: u)\\
  &+ cnt(v \: P_{sw} \: u) + cnt(u \: P_{ws} \: v),
\end{align*}

and

\[
  cnt(u \: P_{sw} \: v) = \sum_{p \in P_{sw}} cnt(u \: p \: v).
\]

Now to rank $u$ against $v$, we compute (4) if possible, otherwise we approximate the probability that $u > v$ using (3). Since the ordering over each pair of adjectives is decided separately using (3) or (4), cycles do exist and transitivity must be enforced. First, we consider a simple integer linear programming formulation, given $N$ adjectives in a cluster where a ranking is known to exist, and define: 

\[
  P_{uv} = \frac{Pr[u > v]}{Pr[v > u]},
\]

so that if $u \ge v$ under $\mathcal{D}_L$ then $P_{uv} \ge 1$:

\begin{align*}
  &{\bf Maximize}\\
  &\sum_{u,v \in \{1,..,N\}} P_{uv} \cdot s_{uv} + P_{vu} \cdot (1 - s_{vu}) \\
  &{\bf s.t}\\
  &(1 - s_{uv}) + (1 - s_{vw}) \geq (1 - s_{uw}),  \\
  &\forall u,v,w \in \{1,...,N\}.\\
\end{align*}

Thus the objective encourages $s_{uv} = 1$ if $u > v$, and $s_{uv} = 0$ otherwise. Overall the objective gives precedent to those pairs where $u$ dominates $v$ the most, while the constraints enforce transitive properties of the ordering. This mixed approximation of $Pr[u > v]$ and $ILP$ gives the best results on Bansal's data, see tables 4 and 5 under ``Markov pairwise mixed ILP." Lastly, in the interest of exploring the trade off between precision versus sparsity of data, we use our approximation of frequency of $u \: p \: v$ in Bansal's formulation. Define:

\begin{align*}
  score(u,v) &= Pr[ v \: P_{sw} \: u \quad or \quad u \: P_{ws} \: v] \\
              &- Pr[ u \: P_{sw} \: v  \quad or \quad v \: P_{ws} \: u],
\end{align*}

so that $score(u,v) > 1$ if $u < v$ under $\mathcal{D}_L$, thereby conforming to Bansal's formulation of the score function in terms of sign. See ``Markov pairwise approximate MILP."




% There are a multitude of issues with regard to the relative meaning of adjectives that need to be resolved in order to assign values. We leave all complications for discussion in the next section and focus on constructing the simplest baseline here. The simplest way forward is to measure outdegree of each vertex so that the value $\gamma_{st}$ of an edge from $s$ to $t$ is:

% \begin{equation}
% \gamma_{st} = \begin{cases} 
%   \frac{\I_{(s,t; \cdot)}}{\sum_{x \in \tV} \I_{(s,x; \cdot)}} & (s,t,\cdot) \in \tE, \\ 
%                                                0 & (s,x,;\cdot)\not\in \tE, \text{for every vertex } x.
% \end{cases}
% \end{equation}

% This measure naturally leads to the following definition of "greater than":

% \theoremstyle{definition}
% \begin{definition}
% Given two adjectives $s$ and $t$, we say $s$ is less intense than $t$ under the assignment of $\gamma$ above, written $s <_{\gamma} t$, if $\gamma_{st} > \gamma_{ts}$. Given three adjectives $s$, $t$ and $r$, we decide $s <_{\gamma} t <_{\gamma} r$ if the following three conditions are satisfied:
%   \begin{align*}
%     \gamma_{st} > \gamma_{ts} \\
%     \gamma_{sr} > \gamma_{rs} \\
%     \gamma_{rt} > \gamma_{tr}.
%   \end{align*}
% \end{definition}

% This definition is consistent with intuition since the edges are intensifiers, an adjective with more outgoing edges is "intensified" more often in common speech than those with a smaller outdegree, therefore it is weaker. If there is data for each of the three above conditions and they are consistent, then we are done. However two common problems surface in practice: missing data, that is for at least one pair of adjectives, both $\gamma_{st}$ and $\gamma_{ts}$ are zero; and/or contradictory data. There are many ways to tackle the data sparsity problem, for now we focus on approximating the probability that an edge should be present given it is missing. At this juncture it is important to remind the reader that the restrictive assumption that any pair of adjectives on the graph can be compared, and therefore there could exist an edge between them even it if it is not observed.

% \subsection{Missing Data.}

% [Need to add a sentence about why ties are not allowed.] Since $\gamma_{st}$ is computed from the number of edges from $s$ to $t$ in the underlying multi-directed graph $\tG$, $\gamma$ can be determined by computing the expected number edges between $s$ and $t$ in $\tG$. There are multiple ways to measure this probability, but only one where data exists to support the expression: by framing this task as a sequential prediction problem. In essence, we ask if we had to place an edge between $s$ and $t$, how many edges would be placed and in what direction only given what we know about the neighborhood of $s$, and similarly for that of $t$. Let $X$ be a random variable ranging over the discrete set $\{-K, K\}$ for some appropriate $K$. In this setting $K$ is chosen to be the maximum number of paraphrases allowed under the data construction processs (In practice 40 is chosen). So if the expected value of $X$ is three, then we would place three edges from $s$ to $t$, if the expected value of $X$ is negative three, then we would place three edges from $t$ to $s$. Now we will determine the expected value of $X$ given that of all $n$ neighbors of $s$: $X_{s1}, \ldots, X_{sn}$, where the neighbors of $s$ are vertices that share an edge with $s$. We assume the probability of placing edges between all vertices to be absolutely independent. Under this simplifying assumption, the expected value of $X$ is simply the sample mean:
% ex
% \begin{equation}
%   \Ex[X ] = \sum_{k \in \{-K,\ldots,K\}} \Prob[X = k] \cdot k,
% \end{equation}

% where:

% \begin{equation}
%   \Prob[X = k] = \frac{  \sum_{X_{st}} \I_{X_{st} = k}  }{\sum_{X_{st}, l} \I_{X_{st} = l}}.
% \end{equation}

% The expected value of $X$ is then rounded up to the nearest natural number, and we denote the missing edges between $s$ and $N$ as $(s, N; -).$  Finally we define $\gamma_{sN}$ as:

% \begin{equation}
%   \tgamma_{sN} = \frac{\I_{(s,N,-)}}{\sum_{x \in \tV} \I_{(s,x; \cdot)}},
% \end{equation}

% and naturally renormalize $\gamma_{st}$ for every $t$ if an edge is observed:

% \begin{equation}
%   \tgamma_{st} = \frac{\I_{(s,t,\cdot)}}{\sum_{x \in \tV} \I_{(s,x; \cdot)} + \I_{(s, N; -)}}.
% \end{equation}

% The same process is repeated for $\gamma_{ts}$ if no edges are observed there. Finally it is important to remind the readers that if either $\gamma_{st}$ or $\gamma_{ts}$ is not zero, we do not approximate $\gamma$ for either vertex. 

% \subsection{Resolving Contradictions.}

% Contradictory data arises naturally from random fluctuations in common speech and inconsistencies due to the PPDB paraphrase system. The most immediate way to force a consistent assignment is to consider the integer linear programming method described in the previous section, reproduced below for ease of reference. Let $b_{st} = 0$ if $s <_{\gamma} t$, and $1$ otherwise; let $V_{st} = \frac{\gamma_{ts}}{\gamma_{st}},$ and pick an assignment over $b$'s that:

% \begin{align*}
%   &{\bf Maximize}\\
%   &\sum_{s,t \in \{1,..,N\}} V_{st} \cdot s_{st} + P_{ts} \cdot (1 - s_{ts}) \\
%   &{\bf s.t}\\
%   &(1 - s_{st}) + (1 - s_{tr}) \geq (1 - s_{sr}),  \\
%   &\forall s,t,r \in \{1,...,N\}.\\
% \end{align*}

% \section{Measure strength via direct comparison.}

% Examining equation (1), it is clear that we have defined how strong $t$ is relative to its neighbors, relative to how strong $s$ is compared to its neighbors. This measure would be consistent if all neighbors of $s$ and $t$ have the same strength, and their neighbors have the same strength, and so on. However if $\gamma_{st} = \gamma_{ts}$, but there is at least one neighbor of $s$ that is stronger than that of $t$ measured by (1), then intuition suggests that $s$ is stronger than $t$, but under (1) they are equal. Instead of recursively computing the strength of $s$ and $t$, we remedy this problem by introducing a second measure that directly compares $s$ and $t$:

% \begin{equation}
%   \delta_{st} = \begin{cases} 
%     \frac{\I_{(s,t; \cdot)}}{\I_{(s,t; \cdot)} + \I_{(t,s; \cdot)}} & (s,t,\cdot) \in \tE \text{ or } (t,s,\cdot) \in \tE \\ 
%     \frac{\tdelta_{st}}{\tdelta_{st} + \tdelta_{ts}} & otherwise,
%   \end{cases}
% \end{equation}

% where $\tdelta$ is approximated using sequential prediction similar to (4):

% \begin{equation}
%   \tdelta_{sN} = \frac{\I_{(s,N,-)}}{\sum_{x \in \tV} \I_{(s,x; \cdot)}}.
% \end{equation}

% Observe that in the absence of information about direct comparisons, we fall back to indepdence assumption between $s$ and $t$, and use their respective neighbors to predict the outcome of $s$ versus $t$. Contradiction is resolved using the integer linear programming formulation in (6). Results are presented below.


% \subsection{Results and Discussion}

% TODO.



  













































