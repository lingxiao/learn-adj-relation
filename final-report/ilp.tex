\subsection{ILP over N-gram Patterns}

In this chapter, we offer an alternate formulation of recovering ranking using monolingual data using integer linear programming (ILP). This chapter stands apart from the previous two chapters, and follows chapter 3 directly. 

\subsection{Problem Formulation}

Recall in Mohit's work, $score(a_i, a_j)$ is zero for most pairs of adjectives due to lack of data, we are motivated to find alternate ways of approximating this value using single sided patterns of form: $\{a_i p *, a_j p *, * p a_i, * p a_j\}$. Loosely speaking, even if we do not observe any $a_i \: p \: a_j$ for some weak-strong pattern $p$, we can still approximate the likelihood of observing this string using the frequency in which $a_i$ appears in front of the the pattern $p$, and the frequency $a_j$ appears behind the pattern $p$. Once we approximate the likelihood for $a_j \: p a_i$ over weak-strong patterns $p$, and similarly for all strong-weak patterns, we can infer whether adjective $a_i$ is weaker than $a_j$ by determining whether $a_i$ is more likely to appear on the weaker side of each phrase. This intuition is naturally expressed in the heuristic:

\[
  score(u) = \frac{ cnt(u \: p_{sw} \: *) + cnt(* \: p_{ws} \: u)  }
                  { cnt(u \: p_{ws} \: *) + cnt(* \: p_{sw} \: u) },
\]

where:
\[cnt( u \: p_{sw} \: *) = \sum_{v \in \mathbf{V}} \sum_{p \in P_{sw}} cnt( u \: p \: v).\]

This score captures the proportion of times $u$ dominates all other words through the patterns given, relative to the proportion of times $u$ is dominated by all other words through the pattern. The results of this ranking is displayed in table 6 in the line ``Markov heuristic."

Now we make the intuition precise. Suppose we have a simple language $\mathbf{L}$ made up only of phrases of the form ``word pattern word" for every word in the unigram set $\mathbf{V}$ and every pattern in table 1, that is we have:

\[ \mathbf{L} = \{ u \: p \: v \quad|\quad u,v \in \mathbf{V}, p \in \mathbf{P_{sw}} \cup \mathbf{P_{ws}} \}. \]

If we can confidently approximate likelihood of each phrase from $\mathbf{L}$ based on N-gram corpus evidence alone then we are done. But because data is sparse, we must fill in the missing counts by assuming the phrases in $\mathbf{L}$ is generated by this Markov process involving two random variables, $V$ whose value range over vocabulary $\mathbf{V}$, and $P$ ranging over the patterns in table 1. The speaker selects a word $u$ according to some distribution $\mathcal{D}_{V}$ over $\mathbf{V}$,  then conditioned on the fact that $u$ is drawn, a phrase $p$ is drawn according to the conditional distribution $\mathcal{D}_{P|V}$. Finally, conditioned on the fact that $p$ is drawn, a word $v$ is sampled from $\mathcal{D}_{V|P}$. The attentive reader will observe that this crude model does not respect word order. In the phrase ``not great (,) just good", our model would generate the phrase ``good not just great". Surpringly this model works well enough to outperform Bansal' method. Now the probability of a phrase is:

\begin{align*}
\mathcal{D}_L = \frac{\mathcal{D}_{V} \: \mathcal{D}_{P|V} \: \mathcal{D}_{V|P}}{Z},\\
\end{align*}

where $Z$ is an appropriate normalization constant. But since we are only interested comparing the relative likelihood of phrases, $Z$ doese not need to be computed. So we have:

\begin{align*}
\mathcal{D}_L = Pr[u \: p \: v] \propto Pr[u] Pr[p | u] Pr[v | p], \quad (1)
\end{align*}

where:
\[Pr[V = u]               = \frac{cnt(u)}{cnt(*)}\]
\[Pr[P = p | V = u]  = \frac{cnt(u \: p \: *)}{cnt(u \: *)}\]
\[Pr[V = v | P = p]  = \frac{cnt(* \: p \: v)}{cnt(* \: p \: *)}\]

where $cnt(*) = \sum_{x \in \mathbf{V}} cnt(x)$. The first distribution is approximated by the one-gram corpus, the second and third distribution by four and five grams. In the interest of not computing normaliztion constant whenever possible, we put the following crude bound on $cnt(u \: *)$:

\begin{align*}
cnt(u \: *) &= \sum_{x} count(u \: x) \leq cnt(u),
\end{align*}

where $x$ is ranges over all suffixes of length three or four. So (1) becomes:\\

\begin{math}
Pr[u \: p \: v] \propto \frac{ cnt(u \: p \: *) \cdot cnt(* \: p \: v) }{ cnt(*) \cdot cnt(* \: p \: *)}. \quad (2) \\
\end{math}

Now define the probability that $u$ is stronger than $v$ under $\mathcal{D}_L$ as:
% define $u$ is probably stronger than $v$ under $\mathcal{D}_L$ as:

\begin{align*}
  Pr[u \: > \: v] &= Pr[ u \: P_{sw} \: v  \quad or \quad v \: P_{ws} \: u] \\
          &= Pr[ u \: P_{sw} \: v] + Pr[ v \: P_{ws} \: u] \\
          &= \sum_{p \in P_{sw}} Pr[u \: p \: v] + \sum_{p \in P_{ws}} Pr[v \: p \: u],
\end{align*}
and similarly for $v > u$. We decide $u$ is stronger than $v$ if:

\begin{align*}
  &Pr[u \: > \: v] \ge Pr[v \: > \: u] \\
  % & Pr[ u \: P_{sw} \: v  \quad or \quad v \: P_{ws} \: u] > Pr[ v \: P_{sw} \: u  \quad or \quad u \: P_{ws} \: v]\\
  % &\implies Pr[ u \: P_{sw} \: v] + Pr[ v \: P_{ws} \: u] > Pr[ v \: P_{sw} \: u] + Pr[ u \: P_{ws} \: v]\\
  &\implies \\
  &\frac{ cnt(u P_{sw} *) \cdot cnt(* P_{sw} v) }{ cnt(*) \cdot cnt(* P_{sw} *)} 
  + \frac{ cnt(v P_{ws} *) \cdot cnt(* P_{ws} u) }{ cnt(*) \cdot cnt(* P_{ws} *)} \\
  &\ge\\
  &\frac{ cnt(v P_{sw} *) \cdot cnt(* P_{sw} u) }{ cnt(*) \cdot cnt(* P_{sw} *)} 
  + \frac{ cnt(u P_{ws} *) \cdot cnt(* P_{ws} v) }{ cnt(*) \cdot cnt(* P_{ws} *)} \\
  % 
  &\implies\\
  % % 
  &\frac{ cnt(u P_{sw} *) \cdot cnt(* P_{sw} v) }{ cnt(* P_{sw} *)} 
  + \frac{ cnt(v P_{ws} *) \cdot cnt(* P_{ws} u) }{ cnt(* P_{ws} *)} \\
  &\ge\\
  &\frac{ cnt(v P_{sw} *) \cdot cnt(* P_{sw} u) }{ cnt(* P_{sw} *)} 
  + \frac{ cnt(u P_{ws} *) \cdot cnt(* P_{ws} v) }{ cnt(* P_{ws} *)}. \quad (3) \\
\end{align*}

Note the normalization constant $cnt(*)$ drops out, and there is a qualitative symmetry in (3) that echos intuition. Since (3) does not output cycles, ranking is done by topological sort; results are reported in table 6 under ``Markov pairwise approximate."  Next, we combine the approximate value from (3) with those directly observed in the corpus. If for some adjective pair $(u,v)$ we observe any one of the following values: $u \: p_{sw} \: v$, $u \: p_{ws} \: v$, $v \: p_{sw} \: u$, or $v \: p_{ws} \: u$, then we can define the probability that $u > v$ under $\mathcal{D}_L$ as:

\begin{align*}
Pr[u > v] &= \frac{cnt(u \: P_{sw} \: v) + cnt(v \: P_{ws} \: u)}{Z} \quad (4)
\end{align*}

where 

\begin{align*}
Z &= cnt(u \: P_{sw} \: v) + cnt(v \: P_{ws} \: u)\\
  &+ cnt(v \: P_{sw} \: u) + cnt(u \: P_{ws} \: v),
\end{align*}

and

\[
  cnt(u \: P_{sw} \: v) = \sum_{p \in P_{sw}} cnt(u \: p \: v).
\]

Now to rank $u$ against $v$, we compute (4) if possible, otherwise we approximate the probability that $u > v$ using (3). Since the ordering over each pair of adjectives is decided separately using (3) or (4), cycles do exist and transitivity must be enforced. First, we consider a simple integer linear programming formulation, given $N$ adjectives in a cluster where a ranking is known to exist, and define: 

\[
  P_{uv} = \frac{Pr[u > v]}{Pr[v > u]},
\]

so that if $u \ge v$ under $\mathcal{D}_L$ then $P_{uv} \ge 1$:

\begin{align*}
  &{\bf Maximize}\\
  &\sum_{u,v \in \{1,..,N\}} P_{uv} \cdot s_{uv} + P_{vu} \cdot (1 - s_{vu}) \\
  &{\bf s.t}\\
  &(1 - s_{uv}) + (1 - s_{vw}) \geq (1 - s_{uw}),  \\
  &\forall u,v,w \in \{1,...,N\}.\\
\end{align*}

Thus the objective encourages $s_{uv} = 1$ if $u > v$, and $s_{uv} = 0$ otherwise. Overall the objective gives precedent to those pairs where $u$ dominates $v$ the most, while the constraints enforce transitive properties of the ordering. This mixed approximation of $Pr[u > v]$ and $ILP$ gives the best results on Bansal's data, see tables 4 and 5 under ``Markov pairwise mixed ILP." Lastly, in the interest of exploring the trade off between precision versus sparsity of data, we use our approximation of frequency of $u \: p \: v$ in Bansal's formulation. Define:

\begin{align*}
  score(u,v) &= Pr[ v \: P_{sw} \: u \quad or \quad u \: P_{ws} \: v] \\
              &- Pr[ u \: P_{sw} \: v  \quad or \quad v \: P_{ws} \: u],
\end{align*}

so that $score(u,v) > 1$ if $u < v$ under $\mathcal{D}_L$, thereby conforming to Bansal's formulation of the score function in terms of sign. See ``Markov pairwise approximate MILP."

\subsection{Results}

\begin{table}
\small
\centering
\begin{tabular}{|l|l|l|}
% 
\hline 
\bf Gold & \bf MILP & \bf Markov ILP \\
\hline
% 
\ \pbox{20cm}{(cool, chilly) \\ $<$ unfriendly \\ $<$ hostile } 
& \pbox{20cm}{unfriendly \\ $<$ cool \\ $<$ hostile \\ $<$ chilly }
& \pbox{20cm}{cool \\ $<$ chilly \\ $<$ unfriendly \\ $<$ hostile } \\
% 
\hline
% 
\ \pbox{20cm}{strong \\ $<$ intense \\ $<$ terrible \\ $<$ overwhelming \\ $<$ violent } 
& \pbox{20cm}{strong \\ $<$ intense \\ $<$ terrible \\ $<$ overwhelming \\ $<$ violent }
& \pbox{20cm}{intense \\ $<$ strong \\ $<$ terrible \\ $<$ violent \\ $<$ overwhelming } \\
% 
\hline
% 
\ \pbox{20cm}{high \\ $<$ higher \\ $<$ soaring }
& \pbox{20cm}{soaring \\ $<$ high \\ $<$ higher \\ }
& \pbox{20cm}{soaring \\ $<$ high \\ $<$ higher \\ }\\
% 
\hline
\end{tabular}
\caption{\label{font-table} The top row displays an example where Bansal's MILP fails to output the correct order (tau = $-0.18$) but Markov Mixed ILP output the correct order modulo ties (tau = $0.91$). The middle row is an example where Bansal' MILP correctly predicted the ranking despite sparse data, only six out of twenty pairs had any N-gram hits. Using Markov assumption the missing data was filled in, but at the cost accuracy. The bottom row shows an instance where both methods fail because there is overwhelming copora evidence that higher is more intense than soaring, revealing the limitations of the pattern-based approach.}
\end{table}

Table 11 reports all results from Mohit's MILP approach, and the ILP reformulation. now we discuss the advantages and drawbacks of each method, specifically the settings in which each method succeeded over another. Looking at table 12 line ``Markov pairwise mixed ILP", we see that the integer linear programming formulation with missing data approximated by (3) enjoy the highest accuracy across all measures. Thereby validating the hypothesis that the Markov condition is sufficiently reasonable assumption to approximate frequency of unseen phrases. Furthermore, it is interesting that ``Markov pairwise approximate" performs almost as well as Bansal's MILP method, even though it relies on the approximation given by (3) alone. This indicates that the linguistic patterns specified is an accurate enough of a predictor of relative adjective strength, so that even crude approximations are powerful predictors. In terms of data complexity, we stress that both (3) and (4) can be calculated from the same data collected by Bansal to compute the score in section 3.1, suggesting that the aforementioned data holds more information about the ordering of adjectives than is used by Bansal' MILP method. Most notably, we outperform Bansal's method without relying on annotator labels of synonyms, which a very restrictive condition indeed.

Now we wish to highlight some specific instances where ``Markov pairwise mixed ILP" did well relative to Bansal' MILP method, and where it faltered. In the interest of fairness, we compare the output of our methods from table 6 only. Bansal's MILP method output 11 rankings where Kendall's tau score was less than zero, while  Markov pairwise mixed ILP only output six. Notably, five out of six bad ILP clusters also appear in Bansal' method, one example of this is given in the bottom row of table 4. In all these cases there is strong corpora evidence leading to the wrong conclusion, but in 94\% of the cases linguistic-pattern points in the right direction. In conclusion, not only is our integer linear program formulation simpler than that of Bansal's, we also outperform the state of the art across all measures in a nontrivial manner. We do so without relying on additional annotations during ranking, which is a very restrictive assumption. Finally, the sample complexity of our approach is comparable to that of Bansal's. 

\begin{table}
\small
\centering
\begin{tabular}{|l|c|c|c|c|c|}
% 
\hline 
\bf Method & \bf Pairwise Accuracy & \bf Avg. $\tau$ & \bf Avg. $|\tau|$ & \bf Avg. $\tau'$ & \bf Avg. $|\tau'|$ \\ 
\hline
% 
Inter-Annotator Agreemen        & 78.0\% & 0.67 & 0.76 & N/A & N/A       \\
Gold Standard                   & 100.0\% & 0.90 & 0.90 & 0.90 & 0.90    \\
\hline
% 
MILP reproduced                 & 68.0\% & 0.55 & 0.64 & 0.41 & 0.54      \\
\hline
% 
Markov heuristic                & 65.0\% & 0.43 & 0.61 & 0.31 & 0.52  \\
Markov pairwise approximate     & 70.0\% & 0.53 & 0.63 & 0.41 & 0.54  \\
Markov pairwise mixed ILP       & 72.0\% & 0.57 & 0.64 & 0.44 & 0.54  \\
Markov MILP                     & 70.0\% & 0.53 & 0.65 & 0.41 & 0.56  \\
% 
\hline
\end{tabular}
\caption{\label{font-table} Main results using Bansal's patterns. Note $\tau$ refers to kendall's $\tau$ with ties, while $\tau'$ referrs to the variant where ties are not considered.}
\end{table}\newpage



































