\section{Logistic Regression Regression}

\subsection{Introduction}

In this section we present a remedial solution to combine our regression models with the pointwise-estimation baseline model, and a more principled solution. Since we are combining the models, our gold standards will now be the original annotated sets composed of with size between two to ten.

\subsection{Remedial Solution}

First we give brief reminder for our baseline model. We defined two possible events: $\pmb{\Omega} = \{s < t, s > t\},$ and after observing a sequence of comparisons between $s$ and $t$: $\pmb{S} = \{ s < t, s < t, \ldots, s > t \ldots \}$, we can ask what is the probability that the next element we will observe is $s < t$. This is a Bernoulli distribution with parameter $p$ and it is well known that the most likely $p$ is simply:

\begin{equation*}
	\Prob[ s < t ] = \frac{|\{ s < t \in \pmb{S} \}|}{|\pmb{S}|}.
\end{equation*}

In the baseline, if $\pmb{S}$ is empty then we defaulted to $\Prob[ s < t ] = \frac{1}{2}$. 

Now we present the remedial solution. Recall in the previous chapter we defined this probability value for the $\hat{y}$ output by elastic net regression:

\[   
\Prob[s < x ] = \left\{
\begin{array}{ll}
      \frac{1}{2} + \epsilon & \hat{y} < \delta \\
      \frac{1}{2} - \epsilon & otherwise,
\end{array} 
\right.
\]

while we used the actual probabilty value $p$ output by the logistic regression model. In the remedial solution, we use the elastic definition defined above, and in the case of logistic regression, we actually discard the value of $p$ and define:
\[   
\Prob[s < x ] = \left\{
\begin{array}{ll}
      \frac{1}{2} + \epsilon & p > \frac{1}{2} \\
      \frac{1}{2} - \epsilon & otherwise.
\end{array} 
\right.
\]

This captures our intuition that the prediction output by the model is less accruate than that of the actual data. Additionally, we also constructed a version of the Turk and Mohit's clusters where ties are removed. We reasoned that since our models are designed to predict ordering, while ties can be interepreted as synonyms, clusters generated without ties may give a more ``fair" representation of how well the models perform. Results are displayed below. 

\begin{table}
\small
\centering
\begin{tabular}{|l|cc|cc|cc|}
	% 
	\hline 
	& \multicolumn{2}{c|}{Elastic Net Regression } 
	& \multicolumn{2}{c|}{$l_1$-Logistic Regression} \\
	\hline 
	\bf Gold Set
	& \bf Pairwise & \bf Avg. $\tau$  
	& \bf Pairwise & \bf Avg. $\tau$  \\ 
	\hline
	% 
	BCS          & 90.0\% & 0.81 & 93.0\%  &  0.85 \\
	Turk         & 75.0\% & 0.62 & 74.0\%  &  0.61 \\
	Turk no-tie  & 81.0\% & 0.63 & 81.0\%  &  0.62 \\
	Mohit        & 74.0\% & 0.61 & 74.0\%  &  0.61 \\
	Mohit no-tie & 76.0\% & 0.52 & 76.0\%  &  0.53 \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table}. Results for the two best models combined with pointwise estimation baseline in the remedial fashion. Note how two models performs comparable across all gold sets. In addition, not the gold clusters with no ties enjoyed a higher pairwise accuracy but suffer a lower $\tau$ value.}
\end{table}

\subsection{Solution with Beta Prior}

In this section we provide a more formal variant of the remedial solution. The heart of the of the problem is that we have some prior belief about the likelihood that one adjective is weaker than another, and an updated belief after observing some data, be it direct comparison or estimation from a model. Since we are modeling each edge a Bernoulli variable with parameter $\theta$ ranging over $[0,1]$, the prior is then a distribution over the Bernoulli $\theta$, this is the Beta distribution. In this next few paragraphs, we give a brief overview of Beta-Binomial model, in particular how it applies to our problem. 

It is well known that the prior the binomial and Bernoulli likelihood function is the Beta distribution with paramters $\beta_1, \beta_2 \in \{1, \ldots \}$, where we have:
	\begin{align*}
		\Prob[\theta | \beta_1, \beta_2 ] 
		&= \frac{\theta^{\beta_1 - 1} (1 - \theta)^{\beta_2 - 1}}{\int_{0}^1 \mu^{\beta_1 - 1} (1 - \mu)^{\beta_2 - 1} d \mu} \\
		&= \frac{\Gamma(\beta_1 + \beta_2)}{\Gamma(\beta_1)\Gamma(\beta_2)} \theta^{\beta_1 - 1} (1-\theta)^{\beta_2 - 1}.
	\end{align*}

The exact form of the $\Gamma$ function is beyond the scope of this introduction but the reader may select any introductory book on statistics for a refresher. 

Now after observing $n$ coin tosses with $h$ heads and $t$ tails for $h + t = n$, the posterior probability over $\theta$ given some prior setting of $\beta_1$ and $\beta_2$ is:
	\begin{align*}
		\Prob[\theta | h, t\beta_1, \beta_2 ] 
		&= \frac{\Prob[ h | n, \theta] \Prob[\theta |, n, \beta_1, \beta_2]}{\Prob[h|n, \beta_1 + \beta_2]} \\
		&\propto \theta^{h + \beta_1 - 1} (1 - \theta)^{t + \beta_2 - 1},
	\end{align*}

note the posterior distribution is also a beta distribution. Now we a have the distribution $\Prob[\theta | h, t\beta_1, \beta_2 ]$, we can return the the pointwise estimation setting and ask what is the likelihood the next toss lands heads, this is exactly the posteriror mean:
	\begin{align*}
		\pmb{E}[\theta | h, t\beta_1, \beta_2 ]  
		&= \int_{0}^1 \theta \Prob[\theta | h, t\beta_1, \beta_2 ] d \theta \\
		&= \frac{\beta_1 + h}{\beta_1 + \beta_2 + n}.
	\end{align*}

Not how the last line appeals strongly to intuition and therefore can be easily used: the expected outcome of the next toss given the prior is simply the prior tosses plus the tosses observed from data. 

In our setting, we fix the ratio of $\beta_1$ and $\beta_2$ so that the prior probability is exactly $1/2$, thus reflecting our ignorance. Note this is consistent with our ad-hoc setting in the remedial solution. The exactly values of $\beta_1$ and $\beta_2$ is a hyperparameter to be tuned, in practice we set $\beta_1 = \beta_2 = 1$. The coin tosses observed from data and the model are also hyperparameters. Note the more confident we are with the data, the larger the values of $h$ and $t$ should be with respect to $\beta_1$ and $\beta_2$. We experimented with a variety of values, and settled on the following settings for $h$ and $t$:
	\begin{enumerate}
		\item If there is an observation, then we use the raw comparison counts between the adjectives as $h$ and $t$
		\item If there is no obervation so we are using the model, we set $h$ to be the probability that the model predicts less than, and $t = 1 - h$.
	\end{enumerate}

In informal terms, we are confident in the quality of direct comparisons, if they can be observed, and not very confident in the prediction of the model. Results for Beta-Binomal model is presented below.  All in all, the best model uses the beta-binomial model to combine direct observations with $l-1$-penalized logistic regression model, the regression model uses top the coin toss probability of 10 most connect neighbors as features. This model achieved $75\%$ pairwise accuracy on Mohit's data set and the Turk set, and a Kendall's $\tau$ value of $0.61$ and $0.62$. After adjusting for ties, the pairwise accuracy on Mohit's data set was $76\%$, which approaches the inter-annotator accuracy of $78\%$, while the pairwise accuracy on the Turks set was $82\%$ after adjusting for ties. 

\begin{table}
\small
\centering
\begin{tabular}{|l|cc|cc|cc|}
	% 
	\hline 
	& \multicolumn{2}{c|}{Elastic Net Regression } 
	& \multicolumn{2}{c|}{$l_1$-Logistic Regression} 
	& \multicolumn{2}{c|}{MILP} \\
	\hline 
	\bf Gold Set
	& \bf Pairwise & \bf Avg. $\tau$  
	& \bf Pairwise & \bf Avg. $\tau$  
	& \bf Pairwise & \bf Avg. $\tau$  \\ 
	\hline
	% 
	BCS          & 90.0\% & 0.81 & \pmb{93.0\%}  &  \pmb{0.85} & 18.0\%  &  0.02 \\
	Turk         & 75.0\% & 0.62 & \pmb{75.0\%}  &  \pmb{0.62} & 25.0\%  &  0.13 \\
	Turk no-tie  & 81.0\% & 0.63 & \pmb{82.0\%}  &  \pmb{0.63} & 19.0\%  &  0.12 \\
	Mohit        & 74.0\% & 0.61 & \pmb{75.0\%}  &  \pmb{0.61} & 69.6\%  &  0.57 \\
	Mohit no-tie & 76.0\% & 0.52 & \pmb{76.0\%}  &  \pmb{0.53} & 68.0\%  &  0.46 \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table}. The first two columns show results for the two best models combined with pointwise estimation baseline using Beta-Binomial model. The third column displays Mohit's MILP method using N-gram data only. The results shows that $l_1$-logistic regression outperformed elastic net regression on most data sets by a small (possibly insignificant) margin, otherwise they are equivalent. In particular, oberve how logistic regression performs just as well on Mohit's set as it does no the Turk set. Furthermore, both model outperform MILP by a non-trivial amount on all gold sets. Finally, note how well the MILP method performs on Mohit's gold cluster, versus how poorly it performs on other gold standards.}
\end{table}\newpage



