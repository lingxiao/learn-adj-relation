\section{Naive Baselines}

\subsection{``Random" Baseline}

In this chapter we present a two simple baselines to estimate $\Prob[s < t]$. If we assume the sign $<$ is a Bernoulli variable so that either $s < t$ or $t < s$, then we can assign a uniform probability of $\frac{1}{2}$ to each outcome. Thus all $\omega \in \pmb{\Omega}$ will have equal probability. This presents an interesting problem to pick the maximum value, since in practice $\pmb{\Omega}$ is represented by a list, Python will pick the maximum over a list of equal valued items by selecting the first element in the list, therefore the ordering of this list is of profound importance. We could default to randomization, but this introduces some uncertainty in our baseline; picking deterministically is a must to prevent pollution from a bad seed. If we sort $\pmb{\Omega}$ \textit{lexicographically}, then there many not be any bias given some arbitrary cluster of words (speaking loosely). But if there is any relationship between intensity of words and their surface form, then this sorting will introduce a huge bias. This exactly the case in any cluster with base-comparative-superlative words, sorting alone guarantees $85\%$ pairwise accuracy on the set with only base-comparative-superlative clusters. The solution is to sort and then \textit{reverse} the list, which will create a equally strong bias against the base-comparative-superlative pairs. But this will ensure any gains we have in accuracy comes from how well we incorporate information, not sorting. In conclusion, the baseline is still contaminated by a bias, but it is the ``worst possible contamination." We hope the reader finds this argument convincing. Finally, we use the same sort and max function in all future methods so the bias will be consistent. 

We test this baseline on three data sets: the original data set annotated by Mohit, the set we constructed using mechanical turks, and finally a set of base comparative superlative adjectives found in the PPDB data base. All results are presented in table 1. Readers who are satisfied with how we constructed the baseline may skip the remarks, those who are unsatisfied may skim and critique.

\begin{remark}
Readers who are committed to a truly random baseline have to contend with the fact that certain methods only have a slight edge relative to others, it is important the measures reflects this edge. Since many clusters are only two or three items long, different randomized outcomes could result in a $\tau$ of $1.0$, $0.333$, or $-1.0$. Therefore randomization will overwhelm any gains in method quality over different runs of the method. In this sense, a deterministic lexicographical ordering presents the best solution to make different methods comparable during development.
\end{remark}

\begin{table}
\small
\centering
\begin{tabular}{|l|ccc|}
	% 
	\hline 
	& \multicolumn{3}{c|}{N-gram} \\
	\hline 
	\bf Test set
	& \bf Pairwise & \bf Avg. $\tau$ & \bf Avg. $|\tau|$ \\
	\hline
	% 
	Mohit & 43.0\% & -0.04 & 0.42 \\ 
	Turk  & 42.0\% & -0.07 & 0.62 \\
	BCS   & 16.0\% & -0.69 & 0.99 \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table} Random baseline. Note how poorly ``randomness" performed on the base-comparative-superlative baseline simply because the lists are sorted lexicographically, while the other sets perform close to random as expected. A high absolute $\tau$ is concerning because this suggests that enough of the cluster in the test sets are of length $2$ so as to make absolute $\tau$ look deceptively high. } 
\end{table}\newpage



\subsection{Pointwise Estimation}

The next simplest baseline to estimate $\Prob[s < t]$ in the spirit of pointwise estimation. Similar to the baseline above, we have two possible events: $\pmb{\Omega} = \{s < t, s > t\},$ and we observe a sequence of comparisons between $s$ and $t$: $\pmb{S} = \{ s < t, s < t, \ldots, s > t \ldots \}$, we can ask what is the probability that the next element we will observe is $s < t$. This is a Bernoulli distribution with parameter $p$ and it is well known that the most likely $p$ is simply:

\begin{equation*}
	\Prob_{\pmb{\Omega}}[ s < t ] = \frac{|\{ s < t \in \pmb{S} \}|}{|\pmb{S}|}.
\end{equation*}

Because this is a baseline, if $\pmb{S}$ is empty then we default to $\Prob[ s < t ] = \frac{1}{2}$, that is to say ``we don't know." 


\subsection{Pointwise Estimation Results}

In this section we analyze the results for where the pointwise estimation baseline fails. The goal of the pointwise baseline is to two fold: (1) assess how much information can be gained by considering pairwise comparisons alone, and (2) understand how the three data sets differ. We estimate the probabilities over three data sets, the N-gram data set composed of all occurrences of $s \mathcal{P} t$ for specified patterns $\mathcal{P}$, the PPDB set of paraphrases, and finally a naive combination of the two sets where the data is simply ``thrown together." All results are presented below in table 1. Readers who do not like to ``lay in bed with the data" may skip to the conclusion, those who are not too smart to count may read each paragraph in detail.

We examine each annotated set over all three data sets. First let us examine the Mohit's test sets over the N-gram data. Over all, all the clusters have some corpus evidence for at least one pair of comparisons. Note similar to Mohit's algorithm, we correctly place ``first, $\ldots$, eight" in the correct order even though we only observe data for some of the comparisons. Furthermore we note that sometimes multiple orderings in $\pmb{\Omega}$ will have equal probabilities, this is not the case here, we picked the unique $\omega^{*}$. In the case of ``close, near, intimate" we only observe data for one out of three possible comparisons, and correctly placed the near to be less than intimate. Now we have two possibilities: ``near $<$ close $<$ intimate" and ``close $<$ near $<$ intimate". Since the list of options are sorted lexicographically and ``near" comes after ``close" lexicographically, we picked ``near $<$ close $<$ intimate". In ``real, solemn, serious, grave", there is strong n-gram corpus evidence that serious is less intense than solemn, thus the order is flipped. Finally, 9 cluster have a negative $\tau$, all of which have strong corpus evidence supporting the ordering under the measure we defined. 

Now we examine Mohit's test over the PPDB data, note it performed only slightly better than the random baseline, suggesting the PPDB corpus has marginal value add. Notably, the cluster ``first, $\ldots$, eight" performed poorly because there is no data in the PPDB graph for these words. The cluster ``close, near, intimate" now has a negative $\tau$ because there are four paraphrases in the PPDB corpus suggesting ``close $<$ near":

\begin{enumerate}
	 \item quite close $\rightarrow$  near
	 \item real close $\rightarrow$  near
	 \item really close $\rightarrow$  near
	 \item so close $\rightarrow$  near
	 \item too close $\rightarrow$  near
	 \item very close $\rightarrow$  near,
\end{enumerate}

and no evidence suggesting ``near $<$ close." In the cluster ``real, solemn, serious, grave" we output an ordering that is of the same pairwise accuracy and $\tau$ value as the N-gram case, but making different mistakes: flipping the order of ``real" and ``solemn" in this case but not because of evidence because there is none, and ``solemn" is after ``real" lexicographically. Overall, $47$ out of the $79$ clusters in the Turk set did not have any data for any pairs of comparisons, this is close to $60\%$ of the data set. In all the clusters with a negative $\tau$, all but one case was due to complete lack of data. A similar story is true in BCS data set, where our method performs better than the ``random" baseline because every time there is evidence for the words, the $<$ sign is flipped to the correct ordering. Finally, we simply note that Mohit's data set did comparably well on the PPDB + N-gram set, this is not surprising since the many of the Mohit's words do not make an appearance in the PPDB data set. 

Now we consider the Turk set over all data sets. On the N-gram data, the Turk set performed no better than random. Over $70\%$ of the clusters in the Turk set do not have any observations in the N-gram set. Thirty five clusters in this set have negative $\tau$'s, and 29 of which are due to lack of data. In other cases, a negative $\tau$ is either due to corpus evidence contradicting the gold set (two cases) or due to ties in the gold set, which our model does not account for. Next we consider the Turk set over the PPDB data, note that $68\%$ pairwise accuracy is a respectable showing if we recall that Mohit achieved $69.2\%$ accuracy with the MILP formulation on his dataset. Over all 38 of the clusters have data for every possible comparison between words ($O(n^2)$ comparisons), however 23 of these clusters have only two words. Five clusters have no observation for any of the links, curiously three out of these five clusters also only contains two words. For the curious the clusters are:

\begin{enumerate}
	\item uncomfortable, embarrassed
	\item shitty, awful
	\item sturdy, intact
	\item vast, plenty, abundant
	\item tough, formidable, daunting.
\end{enumerate}

Suffice it to say the last cluster is a fair description of writing this thesis, we hope the second cluster does not describe the experience of \textit{reading} this thesis. Finally 14 clusters have negative $\tau$'s, two of these clusters have no data (the second and third cluster from the list above, for those who are wondering). The rest have corpus evidence that contradicts annotators. For example, the annotators ranked ``hardworking $<$ tough $<$ tenacious", while the algorithm ranked ``tough $<$ tenacious $<$ hardworking" because in the PPDB corpus we observe:

\begin{enumerate}
	\item very tough $\rightarrow$ tenacious
	\item very tough $\rightarrow$ hardworking
	\item pretty tough $\rightarrow$ hardworking
	\item really tough $\rightarrow$ hardworking
	\item so tough $\rightarrow$ hardworking.
\end{enumerate}

We leave it to the reader's imagination for the interpretation of $<$ in this setting, and which quality should rank higher under $<$. Finally we examine the Turk set over the PPDB + N-gram data set, where our estimation performed as well as we did on Mohit's set. This is encouraging because this suggests that although the two data sets performed drastically differently over the PPDB and N-gram data sets alone, they performed comparably well on the combined data sets. Which confirms our suspicion that the lack of data is the cause of the discrepancy, not how the sets are curated. 

Finally we examine the base comparative superlative set. When test on the N-gram data set, 189 out of 285 clusters have no data according to the N-gram corpus, 64 clusters have data for every pair of comparison. One hundred and eighty two clusters have negative $\tau$'s, 24 of which have data for one comparison out of the $O(n^2)$ possible comparison between words, in all cases corpus evidence placed the words in the correct order, but lexicographical ordering ensured that the overall order of words still has a negative $\tau$. For example, in the cluster ``short, shorter, shortest" we observe $\Prob[ short < shorter ] = 0.99$ but have no observations for the other pairs, so the overall ranking is $shortest < short < shorter$, again due to lexicographical ordering.

Now we examine the BCS set on the PPDB data set. Pairwise accuracy is appreciably higher on this data set because only 101 clusters have no data, while 159 have data between all $O(n^2)$ possible comparisons. Ninety five clusters have negative $\tau$'s, however six of these clusters have data supporting the ranking. The ranking output by our methods are:

\begin{enumerate}
	\item more $<$ many
	\item more $<$ some
	\item more $<$ much
	\item better $<$ well
	\item latest $<$ later
	\item poorest $<$ poorer.
\end{enumerate}

The fact that ``more" makes such a strong showing is curious, in the first cases, there exists just one edge from ``more" to ``many": ``any more $\rightarrow$ many." In the second case there is just one edge again: ``a few more $\rightarrow$ some". In both cases there are no edges going the other way. In the third cases, there are six edges suggesting more is less intense than much:

\begin{enumerate}
	\item a lot more $\rightarrow$ much
	\item considerably more $\rightarrow$ much
	\item little more $\rightarrow$ much
	\item lot  more $\rightarrow$ much
	\item significantly more $\rightarrow$ much
	\item substantially more $\rightarrow$ much,
\end{enumerate}

and just one edge suggesting more is more intense than much: very much $\rightarrow$ more. A sharp eyed reader might immediately ask how connected are the two vertices relative to each other, to satiate your curiosity we report the values here: more has 147 total neighbors, 42 in-neighbors and 115 out-neighbors. Much on the other hand only has 78 neighbors, 23 in-neighbors and 66 out-neighbors. In other words, one vertex may appear to dominate another if we consider the number of edges alone, but may not dominate if we take their overall connectivity in the graph into account. We will use this particular observation to improve our results later. 

Finally we close the chapter by examining the BCS labeled set on the PPDB + N-gram data. Incorporating N-gram data has the immediate consequence that one more cluster now has data for all possible comparisons among adjectives, progress comes in the most incremental steps. Ninety nine clusters still have no data whatsoever, and 93 clusters still have negative $\tau$'s, 81 of which are because there is no data for any pairs. Only 5 of these negative clusters have data between all pairwise comparisons, they are have already been listed above. Notably, ``more" is now correctly classified as less intense than ``much" due to overwhelming N-gram evidence: 67386 edges point from ``much" to ``more", while only 2834 edges point the other way. 

In conclusion, we discovered two problems that led to bad results: lack of data, and bias from pairwise comparisons without regard to neighborhood connectivity. In the first case, when data exists they support the ordering constructed by annotator. In the second case a highly connected vertex may appear to be dominated by a lowly connected vertex, but in actually dominates this vertex. In the next few chapters, we construct multiple solutions that resolves this problem.
 
\begin{table}
\small
\centering
\begin{tabular}{|l|ccc|ccc|ccc|}
	% 
	\hline 
	& \multicolumn{3}{c|}{N-gram} 
	& \multicolumn{3}{c|}{PPDB} 
	& \multicolumn{3}{c|}{PPDB + N-gram} \\
	\hline 
	\bf Test set
	& \bf Pairwise & \bf Avg. $\tau$ & \bf Avg. $|\tau|$ 
	& \bf Pairwise & \bf Avg. $\tau$ & \bf Avg. $|\tau|$ 
	& \bf Pairwise & \bf Avg. $\tau$ & \bf Avg. $|\tau|$ \\ 
	\hline
	% 
	Mohit & \pmb{72.0\%}  & \pmb{0.56}  & \pmb{0.65} & 46.2\% & 0.02 & 0.46 & 71.3\% & 0.53 & 0.66 \\ 
	% 
	Turk  & 47.0\%  & 0.04  & 0.62  & 68.5\% & 0.49 & 0.70 & \pmb{71.0\%} & \pmb{0.55} & \pmb{0.72} \\
	% 
	BCS   & 38.0\%  & -0.24 & 0.92  & 65.5\% & 0.30 & 0.94 & \pmb{66.0\%} & \pmb{0.33} & \pmb{0.95} \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table} Results across all datasets. Observe how N-gram graph only performed slightly better than the base line on base-comparative-superlative dataset.  A similar story holds for the Turk set. However on Mohit's set we already manage to achieve a higher or comparable accuracy across all measures on the N-gram set than what Mohit did in his TACL paper. }
\end{table}

