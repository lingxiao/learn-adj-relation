
\section{Results}

In this section, we discuss the three measures used to evaluate our results, and then compare our approaches against such metrics. 

\subsection{Metrics}

First we consider pairwise accuracy as defined by Bansal \newcite{demelo:13}. If every adjective in each cluster is assigned a numerical ranking $r(a_i)$, then the label of each pair is defined as as: 

\[
  L(a_i, a_j) = \begin{cases}
  > & if \quad r(a_i) > r(a_j)\\
  < & if \quad r(a_i) < r(a_j)\\
  = & if \quad r(a_i) = r(a_j).\\
  \end{cases}
\]

Given gold-standard labels $L_G$ and predicted labels $L_P$, the pairwise accuracy of each cluster of adjectives is the fraction of pairs that are correctly classified:

\[
PW = \frac{ \sum_{i < j} \mathbbm{1}( L_G(a_i,a_j) = L_P(a_i,a_j) )  }{\sum_{i<j} \mathbbm{1} }
\]

Next we assess the rank correlation between the gold-standard and the predicted set. Kendall's tau measures the total number of pairwise inversions \newcite{Kruskal:58}:

\[
\tau = \frac{ P - Q }{\sqrt{  (P + Q + X)(P + Q + Y) }  }. \quad (5)
\]


P measures the number of concordant pairs and Q is the number of discordant pairs, X is the number of pairs tied in the gold ranking, and Y is number of ties in the predicted ranking. The pair $(ai,aj)$ are:

\begin{itemize}
\item concordant if $r_G(a_i) < r_G(a_j)$ and $r_L(a_i) < r_L(a_j)$ or $r_G(a_i)> r_G(a_j)$ and $r_L(a_i) > r_L(a_j)$
\item discordant if $r_G(a_i) < r_G(a_j)$ and $r_L(a_i) > r_L(a_j)$ or $r_G(a_i) > r_G(a_j)$ and $r_L(a_i) < r_L(a_j)$
\item tied if $r_G(a_i) = r_G(a_j)$ and $r_L(a_i) = r_L(a_j)$ or $r_G(a_i) = r_G(a_j)$ and $r_L(a_i) = r_L(a_j)$
\end{itemize}

Since many of our ranking methods do not allow ties, we also consider a variant where the ties are not counted:

\[
  \tau' = \frac{ P - Q }{n \cdot (n-1)/ 2 }, \quad (6)
\]

here $n$ is the number of adjectives in a cluster, and $\frac{n \cdot (n-1)}{2}$ is the total number of unique pairs. In this case the predicted label is discordant w.r.t. gold if the label is flipped, or if the gold-standard pair is a tie. The overall efficacy of each ranking method is captured by finding the average kendall's tau score. Additionally, Bansal observed that sometimes the ordering of adjectives was clear but the annotators would disagreed about which end of the scale was the stronger one, thus absolute kendall's tau is also reported.

During the course of this project we observed that it is possible to outperform certain gold standards under (5). This behavior is highly unexpected and it behooves the reader to consider this concrete example. Suppose our gold standard is: $G = [[a,b],[c]]$, read as: $a$ is tied with $b$, and they both dominate $c$. Then relative to itself, $G$ has three unique pairs:
\[
  pairs = [(a,b),(a,c),(b,c)],
\]
two concordant pairs: $P = [(a,c),(b,c)]$, no discordant pairs, and one tied pair so that both $X$ and $Y$ in (5) are one. Thus Kendall's tau of $G$ with respect to itself is:

\begin{align*}
  \tau &= \frac{2 - 0}{\sqrt{(2 + 0 + 1)(2 + 0 + 1)}} \\
     &= \frac{2}{\sqrt{9}} = \frac{2}{3}.
\end{align*}

Observe in the case of ties, the maximum Kendall's tau is less than $1$. Next consider the ranking $A = [[a],[b],[c]]$, read as $a$ dominates $b$ and $c$, while $b$ dominates $c$. Once again we have two concordant pairs $[(a,c),(b,c)]$ but no discordant pairs by definition, $X = 1$ and now $Y = 0$ because $A$ does not have ties. Thus $A$ with respect to $G$ is:

\begin{align*}
  \tau &= \frac{2 - 0}{\sqrt{(2 + 0 + 1)(2 + 0)}} \\
     &= \frac{2}{\sqrt{6}} > \frac{2}{3}.
\end{align*}

Therefore an algorithms that ranks the adjectives in correct order without ties can actually outperform the gold standard against itself if the gold ranking does have ties. In the interest of fair comparision, we also report how well gold performs against itself in table 5.

\subsection{Analysis}


\begin{table}
\small
\centering
\begin{tabular}{|l|l|l|}
% 
\hline 
\bf Gold & \bf MILP & \bf Markov ILP \\
\hline
% 
\ \pbox{20cm}{(cool, chilly) \\ $<$ unfriendly \\ $<$ hostile } 
& \pbox{20cm}{unfriendly \\ $<$ cool \\ $<$ hostile \\ $<$ chilly }
& \pbox{20cm}{cool \\ $<$ chilly \\ $<$ unfriendly \\ $<$ hostile } \\
% 
\hline
% 
\ \pbox{20cm}{strong \\ $<$ intense \\ $<$ terrible \\ $<$ overwhelming \\ $<$ violent } 
& \pbox{20cm}{strong \\ $<$ intense \\ $<$ terrible \\ $<$ overwhelming \\ $<$ violent }
& \pbox{20cm}{intense \\ $<$ strong \\ $<$ terrible \\ $<$ violent \\ $<$ overwhelming } \\
% 
\hline
% 
\ \pbox{20cm}{high \\ $<$ higher \\ $<$ soaring }
& \pbox{20cm}{soaring \\ $<$ high \\ $<$ higher \\ }
& \pbox{20cm}{soaring \\ $<$ high \\ $<$ higher \\ }\\
% 
\hline
\end{tabular}
\caption{\label{font-table} The top row displays an example where Bansal's MILP fails to output the correct order (tau = $-0.18$) but Markov Mixed ILP output the correct order modulo ties (tau = $0.91$). The middle row is an example where Bansal' MILP correctly predicted the ranking despite sparse data, only six out of twenty pairs had any N-gram hits. Using markov assumption the missing data was filled in, but at the cost accuracy. The bottom row shows an instance where both methods fail because there is overwhelming copora evidence that higher is more intense than soaring, revealing the limitations of the pattern-based approach.}
\end{table}

table 5 reports all results from the different approachs, while table 6 reports results from all approaches incoporating the new patterns from table 2. Now we discuss the advantanges and drawbacks of each method, specifically the settings in which each method succeeded over another. 

First, note that our reproduction of Bansal's MILP is accurate with acceptable errors. However, we were not able to reproduce MILP with synonymy accurately because Bansal relied on synonyms marked by annotators, these annotations were not released in the public codebase accompanying the paper. Instead we attempted to replicate the experiment using synonyms used by wordnet, and observed a marked decrease in accuracy across all measures. 

Looking at table 6 line ``markov pairwise mixed ILP", we see that the integer linear programming formulation with missing data approximated by (3) enjoy the highest accuracy across all measures. In particular the average tau breaks through the $0.6$ barrier and is only six percent away from inter-annotator agreement tau of $0.67$. Pairwise accuracy is also very close to inter-annotator agreement. Thereby validating the hypothesis that the markov condition is sufficiently reasonable assumption to approximate frequency of unseen phrases. Furthermore, it is interesting that ``markov pairwise approximate" performs almost as well as Banal's MILP method, even though it relies on the approximation given by (3) alone. This indicates that the linguistic patterns specified is an accurate enough of a predictor of relative adjective strength, so that even crude approximations are powerful predictors. In terms of data complexity, we stress that both (3) and (4) can be calculated from the same data collected by Bansal to compute the score in section 3.1, suggesting that the aforementioned data holds more information about the ordering of adjectives than is used by Bansal' MILP method. Most notably, we outperform Bansal' method without relying on annotator labels of synonyms, which a very restrictive condition indeed.

Next we compare the results of table 5 to that of 5. Recall in table 6 we incorporated six additional phrases from table 2, we see all methods benefited from the additional patterns, which increased counts of phrases without sacrificing accuracy. However ``markov pairwise mixed ILP" benefited the most across all measures, suggesting that so long as transitivity is enforced, more data leads to better results. 

Now we wish to highlight some specific instances where ``markov pairwised mixed ILP" did well relative to Bansal' MILP method, and where it faltered. In the interest of fairness, we compare the output of our methods from table 6 only. Bansal's MILP method output 11 rankings where Kendall's tau score was less than zero, while  markov pairwise mixed ILP only output six. Notably, five out of six bad ILP clusters also appear in Bansal' method, one example of this is given in the bottom row of table 4. In all these cases there is strong corpora evidence leading to the wrong conclusion, but in 94\% of the cases linguistic-pattern points in the right direction. Next we say a ranking is ``average" if the tau score is between $0$ and $0.6$. $37$ out of $91$ ranking in Bansal's method are just average, while $31$ of markov ILP are average. Markov ILP moved 10 clusters from the average range to the ``good" range, where tau is greater than $0.6$. A tau score is ``great" if it is above $0.8$, 43 of Markov ILP's rankings are great, while only 35 of MILP's are in this range. Finally, $20$ out of $91$ rankings output by MILP achieved a perfect Kendall's tau score, while ILP achieved perfect in $26$ clusters. This suggests that we can expect the markov approximation of pattern co-occurences to improve each category so long as transitivity is enforced. Finally we also found that Bansal's score display a higher variance of $0.18$, while Markov ILP has variance $0.16$, suggesting that our method is more concentrated around the true rankings. Refer to table 4 for specific instances where each method shines. 





\begin{table}
\small
\centering
\begin{tabular}{|l|c|c|c|c|c|}
% 
\hline 
\bf Method & \bf Pariwise Accuracy & \bf Avg. $\tau$ & \bf Avg. $|\tau|$ & \bf Avg. $\tau'$ & \bf Avg. $|\tau'|$ \\ 
\hline
% 
Inter-Annotator Agreement           & 78.0\% & 0.67 & 0.76 & N/A & N/A         \\
Gold Standard                       & 100.0\% & 0.90 & 0.90 & 0.90 & 0.90         \\
\hline
% 
% 
MILP reported                       & 69.6\% & 0.57 & 0.65 & N/A & N/A         \\
MILP with synonymy reported         & 78.2\% & 0.57 & 0.66 & N/A & N/A         \\
\hline
% 
MILP reproduced                     & 68.0\% & 0.55 & 0.64 & 0.41 & 0.54         \\
MILP with synonymy reproduced       & 65.0\% & 0.43 & 0.58 & 0.31 & 0.50         \\
\hline
% 
Markov heuristic                & 65.0\% & 0.43 & 0.61 & 0.31 & 0.52  \\
Markov pairwise approximate     & 70.0\% & 0.53 & 0.63 & 0.41 & 0.54  \\
Markov pairwise mixed ILP       & 72.0\% & 0.57 & 0.64 & 0.44 & 0.54  \\
Markov MILP                     & 70.0\% & 0.53 & 0.65 & 0.41 & 0.56  \\
% 
\hline
\end{tabular}
\caption{\label{font-table} Main results using Bansal's patterns. Note $\tau$ refers to kendall's $\tau$ with ties, while $\tau'$ referrs to the variant where ties are not considered.}
\end{table}

\begin{table}
\small
\centering
\begin{tabular}{|l|c|c|c|c|c|}
% 
\hline 
\bf Method & \bf Pariwise Accuracy & \bf Avg. $\tau$ & \bf Avg. $|\tau|$ & \bf Avg. $\tau'$ & \bf Avg. $|\tau'|$ \\ 
\hline
% 
MILP reproduced                 & 70.0\% & 0.58 & 0.66 & 0.44 & 0.56 \\
MILP with synonymy reproduced   & 65.4\% & 0.43 & 0.58 & 0.31 & 0.50 \\
\hline
% 
Markov heuristic                & 67.8\% & 0.47 & 0.62 & 0.36 & 0.55  \\
Markov pairwise approximate     & 71.0\% & 0.53 & 0.66 & 0.41 & 0.55  \\
Markov pairwise mixed ILP       & \bf 75.0\% & \bf 0.63 & \bf 0.69 & \bf 0.50 & \bf 0.58  \\
Markov MILP                     & 70.0 \% & 0.52 & 0.64 & 0.40 & 0.52  \\
% 
\hline
\end{tabular}
\caption{\label{font-table} Main results using Bansal's patterns and those found in table 2.}
\end{table}


