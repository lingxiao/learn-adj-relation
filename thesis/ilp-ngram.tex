\subsection{ILP over N-gram Patterns}

\subsection{Pairwise Ranking with One Sided Patterns}

Since $score(a_i, a_j)$ is zero for most pairs of adjectives due to lack of data, we are motivated to find alternate ways of approximating this value using single sided patterns of form: $\{a_i p *, a_j p *, * p a_i, * p a_j\}$. Loosely speaking, even if we do not observe any $a_i \: p \: a_j$ for some weak-strong pattern $p$, we can still approximate the liklihood of observing this string using the frequency in which $a_i$ appears in front of the the pattern $p$, and the frequecy $a_j$ appears behind the pattern $p$. Once we approximate the liklihood for $a_j \: p a_i$ over weak-strong patterns $p$, and similarly for all strong-weak patterns, we can infer whether adjective $a_i$ is weaker than $a_j$ by determining whether $a_i$ is more likely to appear on the weaker side of each phrase. This intuition is naturally expressed in the heuristic:

\[
  score(u) = \frac{ cnt(u \: p_{sw} \: *) + cnt(* \: p_{ws} \: u)  }
                  { cnt(u \: p_{ws} \: *) + cnt(* \: p_{sw} \: u) },
\]

where:
\[cnt( u \: p_{sw} \: *) = \sum_{v \in \mathbf{V}} \sum_{p \in P_{sw}} cnt( u \: p \: v).\]

This score captures the proportion of times $u$ dominates all other words through the patterns given, relative to the proportion of times $u$ is dominated by all other words through the pattern. The results of this ranking is displayed in table 6 in the line ``Markov heuristic."

Now we make the intuition precise. Suppose we have a simple language $\mathbf{L}$ made up only of phrases of the form ``word pattern word" for every word in the unigram set $\mathbf{V}$ and every pattern in table 1, that is we have:

\[ \mathbf{L} = \{ u \: p \: v \quad|\quad u,v \in \mathbf{V}, p \in \mathbf{P_{sw}} \cup \mathbf{P_{ws}} \}. \]

If we can confidently approximate liklihood of each phrase from $\mathbf{L}$ based on N-gram corpus evidence alone then we are done. But because data is sparse, we must fill in the missing counts by assuming the phrases in $\mathbf{L}$ is generated by this markov process involving two random variables, $V$ whose value range over vocabulary $\mathbf{V}$, and $P$ ranging over the patterns in table 1. The speaker selects a word $u$ according to some distribution $\mathcal{D}_{V}$ over $\mathbf{V}$,  then conditioned on the fact that $u$ is drawn, a phrase $p$ is drawn according to the conditional distribution $\mathcal{D}_{P|V}$. Finally, conditioned on the fact that $p$ is drawn, a word $v$ is sampled from $\mathcal{D}_{V|P}$. The attentive reader will observe that this crude model does not respect word order. In the phrase ``not great (,) just good", our model would generate the phrase ``good not just great". Surpringly this model works well enough to outperform Bansal' method. Now the probabilty of a phrase is:

\begin{align*}
\mathcal{D}_L = \frac{\mathcal{D}_{V} \: \mathcal{D}_{P|V} \: \mathcal{D}_{V|P}}{Z},\\
\end{align*}

where $Z$ is an appropriate normalization constant. But since we are only interested comparing the relative liklihood of phrases, $Z$ doese not need to be computed. So we have:

\begin{align*}
\mathcal{D}_L = Pr[u \: p \: v] \propto Pr[u] Pr[p | u] Pr[v | p], \quad (1)
\end{align*}

where:
\[Pr[V = u]               = \frac{cnt(u)}{cnt(*)}\]
\[Pr[P = p | V = u]  = \frac{cnt(u \: p \: *)}{cnt(u \: *)}\]
\[Pr[V = v | P = p]  = \frac{cnt(* \: p \: v)}{cnt(* \: p \: *)}\]

where $cnt(*) = \sum_{x \in \mathbf{V}} cnt(x)$. The first distribution is approximated by the one-gram corpus, the second and third distribution by four and five grams. In the interest of not computing normaliztion constant whenever possible, we put the following crude bound on $cnt(u \: *)$:

\begin{align*}
cnt(u \: *) &= \sum_{x} count(u \: x) \leq cnt(u),
\end{align*}

where $x$ is ranges over all suffixes of length three or four. So (1) becomes:\\

\begin{math}
Pr[u \: p \: v] \propto \frac{ cnt(u \: p \: *) \cdot cnt(* \: p \: v) }{ cnt(*) \cdot cnt(* \: p \: *)}. \quad (2) \\
\end{math}

Now define the probability that $u$ is stronger than $v$ under $\mathcal{D}_L$ as:
% define $u$ is probably stronger than $v$ under $\mathcal{D}_L$ as:

\begin{align*}
  Pr[u \: > \: v] &= Pr[ u \: P_{sw} \: v  \quad or \quad v \: P_{ws} \: u] \\
          &= Pr[ u \: P_{sw} \: v] + Pr[ v \: P_{ws} \: u] \\
          &= \sum_{p \in P_{sw}} Pr[u \: p \: v] + \sum_{p \in P_{ws}} Pr[v \: p \: u],
\end{align*}
and similarly for $v > u$. We decide $u$ is stronger than $v$ if:

\begin{align*}
  &Pr[u \: > \: v] \ge Pr[v \: > \: u] \\
  % & Pr[ u \: P_{sw} \: v  \quad or \quad v \: P_{ws} \: u] > Pr[ v \: P_{sw} \: u  \quad or \quad u \: P_{ws} \: v]\\
  % &\implies Pr[ u \: P_{sw} \: v] + Pr[ v \: P_{ws} \: u] > Pr[ v \: P_{sw} \: u] + Pr[ u \: P_{ws} \: v]\\
  &\implies \\
  &\frac{ cnt(u P_{sw} *) \cdot cnt(* P_{sw} v) }{ cnt(*) \cdot cnt(* P_{sw} *)} 
  + \frac{ cnt(v P_{ws} *) \cdot cnt(* P_{ws} u) }{ cnt(*) \cdot cnt(* P_{ws} *)} \\
  &\ge\\
  &\frac{ cnt(v P_{sw} *) \cdot cnt(* P_{sw} u) }{ cnt(*) \cdot cnt(* P_{sw} *)} 
  + \frac{ cnt(u P_{ws} *) \cdot cnt(* P_{ws} v) }{ cnt(*) \cdot cnt(* P_{ws} *)} \\
  % 
  &\implies\\
  % % 
  &\frac{ cnt(u P_{sw} *) \cdot cnt(* P_{sw} v) }{ cnt(* P_{sw} *)} 
  + \frac{ cnt(v P_{ws} *) \cdot cnt(* P_{ws} u) }{ cnt(* P_{ws} *)} \\
  &\ge\\
  &\frac{ cnt(v P_{sw} *) \cdot cnt(* P_{sw} u) }{ cnt(* P_{sw} *)} 
  + \frac{ cnt(u P_{ws} *) \cdot cnt(* P_{ws} v) }{ cnt(* P_{ws} *)}. \quad (3) \\
\end{align*}

Note the normalization constant $cnt(*)$ drops out, and there is a qualitative symmetry in (3) that echos intuition. Since (3) does not output cycles, ranking is done by topological sort; results are reported in table 6 under ``markov pairwise approximate."  Next, we combine the approximate value from (3) with those directly observed in the corpus. If for some adjective pair $(u,v)$ we observe any one of the following values: $u \: p_{sw} \: v$, $u \: p_{ws} \: v$, $v \: p_{sw} \: u$, or $v \: p_{ws} \: u$, then we can define the probability that $u > v$ under $\mathcal{D}_L$ as:

\begin{align*}
Pr[u > v] &= \frac{cnt(u \: P_{sw} \: v) + cnt(v \: P_{ws} \: u)}{Z} \quad (4)
\end{align*}

where 

\begin{align*}
Z &= cnt(u \: P_{sw} \: v) + cnt(v \: P_{ws} \: u)\\
  &+ cnt(v \: P_{sw} \: u) + cnt(u \: P_{ws} \: v),
\end{align*}

and

\[
  cnt(u \: P_{sw} \: v) = \sum_{p \in P_{sw}} cnt(u \: p \: v).
\]

Now to rank $u$ against $v$, we compute (4) if possible, otherwise we approximate the probability that $u > v$ using (3). Since the ordering over each pair of adjectives is decided separately using (3) or (4), cycles do exist and transitivity must be enforced. First, we consider a simple integer linear programming formulation, given $N$ adjectives in a cluster where a ranking is known to exist, and define: 

\[
  P_{uv} = \frac{Pr[u > v]}{Pr[v > u]},
\]

so that if $u \ge v$ under $\mathcal{D}_L$ then $P_{uv} \ge 1$:

\begin{align*}
  &{\bf Maximize}\\
  &\sum_{u,v \in \{1,..,N\}} P_{uv} \cdot s_{uv} + P_{vu} \cdot (1 - s_{vu}) \\
  &{\bf s.t}\\
  &(1 - s_{uv}) + (1 - s_{vw}) \geq (1 - s_{uw}),  \\
  &\forall u,v,w \in \{1,...,N\}.\\
\end{align*}

Thus the objective encourages $s_{uv} = 1$ if $u > v$, and $s_{uv} = 0$ otherwise. Overall the objective gives precedent to those pairs where $u$ dominates $v$ the most, while the constraints enforce transitive properties of the ordering. This mixed approximation of $Pr[u > v]$ and $ILP$ gives the best results on Bansal's data, see tables 4 and 5 under ``Markov pairwise mixed ILP." Lastly, in the interest of exploring the trade off between precision versus sparsity of data, we use our approximation of frequency of $u \: p \: v$ in Bansal's formulation. Define:

\begin{align*}
  score(u,v) &= Pr[ v \: P_{sw} \: u \quad or \quad u \: P_{ws} \: v] \\
              &- Pr[ u \: P_{sw} \: v  \quad or \quad v \: P_{ws} \: u],
\end{align*}

so that $score(u,v) > 1$ if $u < v$ under $\mathcal{D}_L$, thereby conforming to Bansal's formulation of the score function in terms of sign. See ``Markov pairwise approximate MILP."

\subsection{Ranking using PPDB Graph.}

This section describes the results of of ranking adjectives based on the PPDB graph, and explores various means of combining N-gram based data with paraphrases from the PPDB graph. We test the results on two sets of ranked clusters, those provided by Mohit Bansal, and a new hand curated set of 76 clusters. 

